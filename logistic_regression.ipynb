{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train = mnist_trainset.data.float() / 255.0  # Normalize and convert to float\n",
    "X_train = X_train.view(X_train.shape[0], -1)   # Flatten the images\n",
    "y_train = mnist_trainset.targets               # Labels\n",
    "\n",
    "# Transform the test data\n",
    "X_val = mnist_testset.data.float() / 255.0    # Normalize and convert to float\n",
    "X_val = X_val.view(X_val.shape[0], -1)        # Flatten the images\n",
    "y_val = mnist_testset.targets                 # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size=64):\n",
    "        # Creating DataLoader for batching\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                # Forward pass\n",
    "                y_pred_train = self.forward(X_batch)\n",
    "                loss_train = self.loss(y_pred_train, y_batch)\n",
    "\n",
    "                # Zero gradients\n",
    "                if self.w.grad is not None:\n",
    "                    self.w.grad.zero_()\n",
    "                if self.b.grad is not None:\n",
    "                    self.b.grad.zero_()\n",
    "\n",
    "                # Backward pass\n",
    "                loss_train.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                with torch.no_grad():\n",
    "                    self.w -= learning_rate * self.w.grad\n",
    "                    self.b -= learning_rate * self.b.grad\n",
    "\n",
    "            # Validation\n",
    "            with torch.no_grad():\n",
    "                y_pred_val = self.forward(X_val)\n",
    "                loss_val = self.loss(y_pred_val, y_val)\n",
    "\n",
    "            # Store losses\n",
    "            train_losses.append(loss_train.item())\n",
    "            val_losses.append(loss_val.item())\n",
    "\n",
    "            # Print loss every few epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss_train.item()}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.unique().shape[0]\n",
    "\n",
    "batch_size = 64\n",
    "X_batch = X_train[:batch_size]\n",
    "y_batch = y_train[:batch_size]\n",
    "\n",
    "W = torch.randn(size=(X_batch.shape[1], num_classes), requires_grad=True)\n",
    "b = torch.zeros(size=(1,1), requires_grad=True)\n",
    "\n",
    "O = X_batch @ W + b\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "y_pred = softmax(O)\n",
    "\n",
    "#def loss(y_pred, y_train):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.one_hot(y_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\woute\\Documents\\GitHub\\deep_learning_from_scratch\\logistic_regression.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/woute/Documents/GitHub/deep_learning_from_scratch/logistic_regression.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mlog(y_batch)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "y_pred * torch.log(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8930, 0.0010, 0.0000, 0.0000, 0.0000, 0.0000, 0.1060,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8230, 0.0000, 0.1750, 0.0020,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.3970, 0.0000, 0.0000, 0.1020, 0.5000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9940,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0010, 0.0000, 0.0000, 0.0000, 0.9140, 0.0850,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0390, 0.0000, 0.0000, 0.0000, 0.9590, 0.0020,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.6270, 0.0000, 0.0000, 0.0000, 0.3730, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0170, 0.0000, 0.0000, 0.0000, 0.0020, 0.9810,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9750, 0.0000, 0.0000, 0.0000, 0.0170, 0.0070,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0020, 0.0000, 0.0020, 0.2530, 0.0030, 0.1860, 0.5550,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9870, 0.0000, 0.0010, 0.0000, 0.0080, 0.0040,\n",
       "         0.0000],\n",
       "        [0.9830, 0.0000, 0.0130, 0.0030, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2140, 0.0110, 0.0000, 0.0000, 0.3280, 0.4480,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0010, 0.0220, 0.0820, 0.0390, 0.8050, 0.0510,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.9720, 0.0000, 0.0000, 0.0150, 0.0000, 0.0010, 0.0090, 0.0000, 0.0040,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0350, 0.0000, 0.0440, 0.9190, 0.0000, 0.0010, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0310, 0.0020, 0.0000, 0.0000, 0.0000, 0.0790, 0.8880,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0250, 0.9750, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0010, 0.0000, 0.9310, 0.0000, 0.0680, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0240, 0.0000, 0.0000, 0.0000, 0.0000, 0.0310, 0.9450,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0940, 0.0030, 0.0000, 0.0000, 0.0000, 0.0090, 0.8940,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000],\n",
       "        [0.0580, 0.0000, 0.0000, 0.0030, 0.0000, 0.6490, 0.0010, 0.2890, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0250, 0.0000, 0.0000, 0.0000, 0.0670, 0.9080,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.5190, 0.0000, 0.0000, 0.0000, 0.3940, 0.0870,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7030, 0.2920, 0.0000, 0.0000, 0.0050, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0320, 0.0000, 0.0000, 0.0000, 0.0000, 0.9680,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.5700, 0.0000, 0.0570, 0.0000, 0.0060, 0.0010,\n",
       "         0.3660],\n",
       "        [0.0000, 0.0000, 0.2770, 0.0000, 0.0000, 0.0000, 0.0000, 0.0370, 0.6860,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9890, 0.0000, 0.0000, 0.0110,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8250, 0.0000, 0.1740, 0.0010,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5250, 0.0000, 0.0000, 0.4740,\n",
       "         0.0000],\n",
       "        [0.0260, 0.0000, 0.0000, 0.0080, 0.0000, 0.0000, 0.4210, 0.5450, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0010, 0.0000, 0.9990, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0020,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000],\n",
       "        [0.1860, 0.0000, 0.0000, 0.2070, 0.0170, 0.2850, 0.0000, 0.3000, 0.0010,\n",
       "         0.0060],\n",
       "        [0.0000, 0.0000, 0.9990, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0010,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.5980, 0.0000, 0.0000, 0.0000, 0.2400, 0.1620,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9500, 0.0100, 0.0000, 0.0190, 0.0200,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1070, 0.0000, 0.0000, 0.0000, 0.8620, 0.0310,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9810, 0.0000, 0.0000, 0.0190,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9560, 0.0000, 0.0170, 0.0000, 0.0200, 0.0080,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0010, 0.9540, 0.0000, 0.0000, 0.0000, 0.0320, 0.0120,\n",
       "         0.0000],\n",
       "        [0.0040, 0.0000, 0.0160, 0.0000, 0.0000, 0.9420, 0.0030, 0.0000, 0.0350,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4130, 0.0000, 0.0000, 0.0000, 0.5790, 0.0070,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1640, 0.0000, 0.8230, 0.0000, 0.0130, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0030, 0.0000, 0.0000, 0.0000, 0.0810, 0.9160,\n",
       "         0.0000],\n",
       "        [0.0060, 0.0390, 0.0000, 0.3560, 0.0040, 0.0000, 0.0000, 0.0270, 0.5690,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0020, 0.0000, 0.0000, 0.0000, 0.0030, 0.9950,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0070, 0.0000, 0.9850, 0.0080,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0020, 0.0690, 0.1980, 0.0000, 0.0000, 0.6890, 0.0420,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9980,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0420, 0.0000, 0.0000, 0.4860, 0.4690, 0.0000,\n",
       "         0.0030],\n",
       "        [0.0000, 0.0000, 0.5620, 0.0010, 0.0030, 0.0000, 0.0000, 0.3990, 0.0350,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9970, 0.0000, 0.0000, 0.0030,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0070, 0.0000, 0.0000, 0.0000, 0.7930, 0.2000,\n",
       "         0.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(y_pred * 1000) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(O).sum(axis=1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = O \n",
    "X_exp = torch.exp(X)\n",
    "X_exp.sum(1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6460e+03, 6.1084e+04, 5.0361e+04, 1.9378e+04, 6.9996e+10, 2.7343e+06,\n",
       "        1.8150e+07, 3.1560e+05, 1.5547e+07, 1.3126e+10],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(O).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9459e-07, 2.9707e-05, 2.7486e-14, 2.0638e-09, 5.4338e-05, 1.8297e-03,\n",
       "         6.9229e-16, 4.5132e-11, 3.5651e-11, 1.9131e-15],\n",
       "        [3.4086e-04, 3.4287e-07, 3.1337e-14, 4.3773e-05, 4.3026e-10, 2.1644e-05,\n",
       "         1.4578e-07, 3.4217e-10, 1.2311e-05, 8.6288e-07],\n",
       "        [1.4064e-04, 1.1862e-04, 1.6303e-03, 1.5537e-10, 5.9690e-04, 3.4065e-11,\n",
       "         8.7321e-04, 5.2101e-06, 2.0020e-08, 2.0865e-05],\n",
       "        [5.7586e-09, 1.0304e-06, 3.7472e-01, 1.3343e-02, 1.2079e-06, 7.7167e-07,\n",
       "         3.8771e-08, 1.2865e-01, 1.1037e-07, 9.4395e-08],\n",
       "        [5.1089e-08, 4.3240e-06, 1.9610e-09, 6.6191e-13, 6.6747e-09, 4.3662e-11,\n",
       "         2.0770e-15, 2.6961e-07, 2.1812e-04, 2.0547e-07],\n",
       "        [5.7922e-08, 1.3082e-07, 6.9564e-09, 1.3455e-12, 4.7819e-12, 1.0093e-04,\n",
       "         1.5863e-11, 9.5883e-08, 2.6991e-08, 2.5376e-06],\n",
       "        [6.3449e-07, 4.2344e-02, 9.3502e-04, 4.1483e-04, 5.1299e-06, 1.1537e-07,\n",
       "         1.2720e-12, 7.8954e-07, 4.1178e-06, 1.0019e-14],\n",
       "        [3.1386e-09, 1.3324e-07, 7.3002e-04, 1.1108e-09, 1.5043e-05, 2.1595e-10,\n",
       "         6.3994e-08, 1.9092e-07, 4.9946e-11, 1.7883e-12],\n",
       "        [2.7646e-03, 1.7075e-02, 1.0760e-06, 1.5581e-05, 2.5693e-08, 4.8080e-08,\n",
       "         1.0927e-09, 9.1453e-08, 2.8769e-06, 1.5766e-12],\n",
       "        [2.2582e-03, 1.7532e-02, 2.2125e-12, 8.3343e-06, 5.7337e-09, 1.3294e-06,\n",
       "         1.8919e-07, 4.9402e-06, 8.6607e-05, 1.3628e-07],\n",
       "        [5.1522e-07, 5.7205e-07, 1.6146e-07, 6.5462e-14, 1.3617e-07, 4.5738e-04,\n",
       "         8.7061e-10, 1.9224e-06, 5.9152e-09, 2.8122e-14],\n",
       "        [2.6858e-06, 4.1366e-01, 2.6482e-08, 3.4515e-02, 2.5290e-11, 3.0611e-09,\n",
       "         5.7082e-09, 1.1368e-03, 6.5822e-08, 5.2572e-05],\n",
       "        [1.2156e-04, 2.3452e-07, 3.4399e-08, 7.2825e-10, 5.1377e-05, 2.7937e-02,\n",
       "         1.3820e-09, 5.7428e-04, 5.6616e-06, 7.6306e-13],\n",
       "        [9.7770e-04, 1.8153e-05, 7.5532e-07, 2.2368e-08, 2.0523e-10, 4.1084e-07,\n",
       "         2.4785e-16, 1.4108e-09, 2.3192e-07, 7.9754e-10],\n",
       "        [4.5701e-05, 2.5624e-02, 2.8996e-07, 7.7749e-04, 2.9151e-10, 3.8059e-09,\n",
       "         4.9718e-12, 1.8499e-06, 8.7067e-07, 9.1459e-10],\n",
       "        [2.8849e-07, 7.6524e-06, 1.3911e-15, 7.2031e-07, 1.3808e-06, 3.8163e-04,\n",
       "         6.3608e-15, 2.7637e-10, 2.5649e-01, 5.7491e-02],\n",
       "        [3.8784e-06, 1.7063e-06, 4.1010e-06, 1.9728e-05, 1.1973e-11, 2.6131e-04,\n",
       "         4.8203e-14, 1.9290e-08, 1.3629e-10, 3.5371e-14],\n",
       "        [1.6316e-08, 4.4945e-01, 6.5248e-14, 1.1977e-05, 7.2916e-09, 5.9247e-09,\n",
       "         2.0315e-08, 4.0443e-02, 3.6422e-04, 9.0244e-10],\n",
       "        [1.7989e-04, 1.2127e-06, 7.2278e-09, 8.6443e-08, 7.5042e-11, 7.5118e-09,\n",
       "         1.0290e-04, 1.0383e-05, 1.5622e-09, 3.9888e-10],\n",
       "        [1.1040e-03, 1.3464e-07, 2.7532e-12, 9.4723e-10, 6.8051e-06, 1.5297e-06,\n",
       "         4.2868e-10, 7.9132e-07, 1.3337e-01, 1.3058e-08],\n",
       "        [1.0459e-10, 2.4986e-11, 3.1447e-02, 5.4315e-16, 2.4027e-03, 3.6698e-11,\n",
       "         2.2711e-06, 6.3224e-08, 3.1991e-08, 2.0579e-11],\n",
       "        [2.6547e-08, 4.7546e-08, 3.6740e-13, 9.3961e-04, 4.4939e-07, 4.3152e-04,\n",
       "         1.3538e-06, 1.1284e-09, 4.2353e-02, 4.6846e-03],\n",
       "        [7.2766e-04, 5.1220e-06, 2.4944e-08, 4.0976e-09, 7.9796e-11, 1.8920e-07,\n",
       "         4.8662e-10, 4.7043e-04, 8.5927e-04, 1.7294e-08],\n",
       "        [1.0802e-10, 3.8475e-07, 4.0921e-01, 1.6397e-02, 2.9236e-06, 3.3516e-07,\n",
       "         2.1911e-09, 2.5570e-02, 1.9105e-07, 1.3998e-07],\n",
       "        [3.0830e-04, 9.8024e-10, 8.1706e-08, 1.6042e-10, 5.0110e-11, 6.9915e-02,\n",
       "         3.1775e-10, 1.0022e-11, 1.0901e-01, 1.9881e-11],\n",
       "        [4.2081e-07, 2.7146e-05, 2.3078e-05, 7.5323e-08, 3.6230e-05, 3.7975e-07,\n",
       "         2.2981e-08, 3.4756e-04, 1.6630e-09, 3.4135e-07],\n",
       "        [3.6230e-04, 1.9813e-05, 3.9897e-03, 2.1369e-10, 6.4411e-07, 3.9131e-11,\n",
       "         7.6824e-12, 1.0455e-07, 3.8213e-11, 6.9031e-07],\n",
       "        [5.1582e-12, 4.3100e-06, 9.8426e-09, 6.1632e-09, 9.3545e-01, 2.4232e-02,\n",
       "         4.3348e-08, 3.6053e-05, 1.3279e-09, 4.5869e-17],\n",
       "        [5.1890e-05, 1.2060e-05, 7.1054e-10, 4.2703e-10, 1.2874e-05, 1.4119e-02,\n",
       "         3.7177e-16, 1.7501e-08, 7.7833e-07, 1.0115e-08],\n",
       "        [9.2116e-07, 1.7827e-03, 3.5636e-05, 1.4031e-03, 5.0484e-11, 3.5555e-03,\n",
       "         1.1218e-15, 1.0945e-04, 9.7650e-05, 2.0169e-03],\n",
       "        [9.1903e-05, 2.3187e-07, 3.2303e-13, 2.0249e-07, 8.3276e-05, 4.8361e-07,\n",
       "         1.5146e-14, 5.2700e-02, 6.2402e-08, 1.0910e-15],\n",
       "        [3.9217e-05, 9.8137e-04, 6.1246e-10, 2.1159e-02, 6.4024e-08, 4.3053e-11,\n",
       "         4.4558e-06, 4.3099e-02, 9.4192e-09, 4.0553e-10],\n",
       "        [9.2582e-01, 6.9129e-06, 1.2712e-10, 2.7840e-07, 4.1642e-12, 6.3210e-08,\n",
       "         2.9486e-12, 3.1516e-09, 1.4156e-06, 1.4182e-11],\n",
       "        [1.6290e-10, 5.7435e-05, 1.8010e-12, 7.2172e-08, 2.1992e-10, 1.5127e-04,\n",
       "         6.6754e-09, 3.5344e-05, 4.5284e-01, 3.6467e-07],\n",
       "        [7.0776e-06, 1.9341e-09, 3.7196e-15, 9.9846e-09, 6.2618e-04, 1.9331e-08,\n",
       "         9.9833e-01, 1.2653e-04, 1.0158e-06, 1.1196e-07],\n",
       "        [9.0917e-07, 6.6661e-05, 1.6192e-08, 6.0632e-06, 6.0933e-07, 1.8827e-01,\n",
       "         1.3606e-06, 4.3565e-04, 9.7751e-07, 2.4461e-09],\n",
       "        [1.3668e-02, 5.5270e-04, 2.6749e-11, 6.2595e-10, 8.2825e-08, 1.1089e-09,\n",
       "         5.7807e-12, 2.7932e-07, 1.1690e-04, 2.2158e-12],\n",
       "        [2.6914e-07, 7.0524e-13, 1.2449e-18, 5.6439e-08, 1.0787e-07, 8.4728e-06,\n",
       "         4.8415e-04, 6.2684e-09, 2.6721e-04, 1.1645e-04],\n",
       "        [1.7990e-08, 3.3696e-05, 2.4537e-08, 1.9186e-07, 3.9151e-08, 3.3774e-08,\n",
       "         8.7310e-16, 2.6735e-07, 3.8291e-09, 4.6176e-18],\n",
       "        [3.4682e-02, 5.6507e-05, 5.9493e-12, 4.2161e-10, 9.1313e-11, 1.7217e-08,\n",
       "         4.1679e-14, 3.2934e-08, 2.7119e-06, 1.0573e-08],\n",
       "        [1.5950e-03, 9.0720e-06, 1.3649e-05, 4.8851e-05, 2.0060e-07, 1.5336e-07,\n",
       "         2.1172e-10, 5.2494e-08, 4.8943e-06, 1.5296e-08],\n",
       "        [9.3386e-08, 5.4710e-05, 9.0302e-08, 1.2404e-07, 8.6156e-08, 1.8683e-06,\n",
       "         9.3022e-12, 5.7777e-04, 1.5839e-06, 7.0618e-06],\n",
       "        [2.0415e-03, 1.0462e-03, 2.4674e-09, 2.2226e-08, 2.5371e-08, 6.3941e-07,\n",
       "         2.3798e-12, 8.6845e-07, 4.7727e-06, 5.3170e-10],\n",
       "        [1.0189e-05, 9.2256e-07, 2.7975e-08, 1.0502e-11, 1.0366e-11, 1.4243e-07,\n",
       "         1.0335e-12, 1.6160e-03, 1.6662e-06, 2.0129e-09],\n",
       "        [3.4880e-09, 1.8424e-05, 1.0340e-04, 3.4183e-06, 7.5090e-04, 8.4340e-09,\n",
       "         2.1404e-05, 4.0461e-05, 1.6209e-07, 8.1417e-16],\n",
       "        [4.8993e-09, 8.0086e-08, 1.0022e-11, 3.1486e-16, 2.4503e-09, 1.1774e-10,\n",
       "         8.0059e-15, 1.2515e-05, 7.2375e-07, 1.4158e-04],\n",
       "        [2.0578e-06, 1.8485e-04, 6.5363e-06, 2.4161e-08, 5.7729e-03, 3.9367e-08,\n",
       "         2.2257e-09, 3.2629e-07, 3.0423e-09, 1.0044e-05],\n",
       "        [1.9157e-03, 5.2760e-07, 1.1702e-16, 1.4141e-01, 5.1213e-10, 9.2710e-06,\n",
       "         5.8410e-07, 2.6608e-09, 1.9940e-08, 1.8364e-11],\n",
       "        [6.5033e-06, 2.0611e-02, 4.0274e-05, 2.6397e-09, 1.0152e-06, 4.5477e-04,\n",
       "         1.8797e-10, 1.8991e-08, 4.7806e-07, 3.2635e-12],\n",
       "        [8.9675e-11, 1.1723e-07, 1.4608e-09, 2.0704e-09, 3.5943e-06, 3.1602e-06,\n",
       "         2.3100e-06, 1.6331e-05, 5.4927e-07, 1.7424e-17],\n",
       "        [5.2121e-03, 1.1448e-07, 8.4666e-08, 3.5901e-05, 1.7387e-08, 6.3609e-01,\n",
       "         3.8116e-07, 1.9517e-04, 2.7927e-06, 1.1189e-10],\n",
       "        [7.5771e-09, 1.7969e-09, 8.3782e-09, 7.4157e-01, 3.5611e-06, 1.6496e-05,\n",
       "         3.9722e-12, 2.1039e-11, 2.9071e-06, 9.7004e-09],\n",
       "        [2.9372e-10, 3.5107e-12, 3.2534e-11, 4.9556e-07, 3.8695e-03, 2.1599e-07,\n",
       "         1.5775e-15, 6.2545e-14, 1.0840e-08, 4.1545e-10],\n",
       "        [4.8359e-03, 7.2576e-08, 9.7184e-07, 1.9650e-05, 1.2516e-07, 1.1578e-09,\n",
       "         7.4733e-08, 3.4136e-07, 7.6557e-05, 6.0631e-11],\n",
       "        [5.5532e-08, 3.9374e-03, 2.0898e-10, 9.4915e-13, 1.1904e-06, 6.5446e-13,\n",
       "         2.1490e-12, 6.1748e-07, 2.6265e-09, 6.2160e-07],\n",
       "        [1.7854e-06, 4.5769e-03, 1.2841e-09, 5.0788e-06, 6.1510e-04, 5.4467e-06,\n",
       "         3.1757e-14, 6.9828e-01, 1.3382e-06, 2.5340e-02],\n",
       "        [1.6172e-05, 7.2791e-05, 3.1294e-10, 2.4954e-10, 9.7595e-04, 3.1733e-02,\n",
       "         2.5103e-13, 8.4149e-11, 1.3427e-03, 9.1011e-01],\n",
       "        [6.4553e-04, 1.3027e-09, 8.2653e-12, 7.6017e-12, 2.9654e-10, 4.9921e-08,\n",
       "         6.3934e-15, 8.0263e-09, 6.5176e-04, 6.4858e-06],\n",
       "        [3.5268e-13, 2.1539e-09, 1.3687e-01, 2.6630e-13, 4.8449e-02, 2.6962e-07,\n",
       "         5.4510e-09, 1.5809e-09, 2.9865e-06, 8.4331e-12],\n",
       "        [2.0884e-08, 8.3731e-09, 2.5913e-02, 2.1492e-05, 2.1640e-07, 1.7674e-06,\n",
       "         1.3038e-07, 5.2889e-03, 1.1627e-10, 7.7644e-07],\n",
       "        [6.0347e-07, 3.7061e-06, 1.3508e-02, 2.7732e-02, 2.0812e-04, 1.2817e-06,\n",
       "         1.5233e-04, 4.5538e-07, 6.2247e-09, 6.0349e-08],\n",
       "        [1.3444e-07, 1.5470e-06, 8.0599e-04, 2.5535e-08, 2.2332e-10, 8.0149e-11,\n",
       "         4.5151e-08, 1.3899e-04, 1.8008e-03, 1.8828e-09],\n",
       "        [1.0375e-05, 3.8956e-07, 2.1451e-11, 1.3152e-11, 1.2161e-06, 3.0905e-12,\n",
       "         2.1045e-05, 2.9072e-06, 4.0729e-08, 1.5125e-10],\n",
       "        [4.8584e-07, 3.0872e-08, 3.4015e-15, 9.3823e-05, 2.1046e-11, 4.6471e-06,\n",
       "         5.6470e-08, 7.1911e-05, 2.2485e-08, 1.9184e-08]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(O) / torch.exp(O).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.9480e+10, 1.5522e+10, 6.3992e+05, 1.2818e+05, 1.0198e+08, 3.4120e+05,\n",
       "        1.0586e+08, 3.8687e+11, 3.4325e+04, 2.4640e+04, 4.1829e+10, 1.2867e+04,\n",
       "        3.5797e+06, 5.2261e+09, 1.5880e+04, 9.4111e+06, 7.0931e+07, 7.1171e+07,\n",
       "        3.9531e+06, 1.9748e+05, 3.6325e+10, 9.0758e+13, 1.2530e+04, 2.2933e+05,\n",
       "        9.4702e+05, 1.3110e+09, 7.6559e+04, 2.7408e+12, 3.6465e+15, 4.0158e+05,\n",
       "        4.4650e+06, 9.1181e+07, 4.1548e+06, 1.3397e+05, 1.2018e+08, 1.9002e+03,\n",
       "        8.4377e+09, 1.7782e+10, 4.0375e+13, 5.8197e+08, 1.8379e+04, 7.0598e+08,\n",
       "        7.6529e+03, 6.5472e+05, 2.4880e+05, 1.4689e+07, 5.6751e+05, 1.4192e+03,\n",
       "        8.9766e+02, 5.2856e+11, 7.1630e+03, 2.6258e+14, 6.1695e+11, 1.4877e+05,\n",
       "        8.0811e+10, 4.5880e+10, 6.9909e+15, 1.7057e+04, 2.1248e+07, 6.5082e+05,\n",
       "        2.3695e+12, 2.1055e+03, 3.0712e+09, 1.4262e+16],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(O).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.4329e+12, 7.2418e+10, 1.1214e+05, 1.7907e+01, 1.0330e+12, 6.4195e+08,\n",
       "        5.3815e+06, 4.8606e+06, 6.7338e+02, 2.2498e+05],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(O).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8613.8633], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(o).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call apply_() on Variable that requires grad. Use var.detach().apply_() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\woute\\Documents\\GitHub\\deep_learning_from_scratch\\logistic_regression.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/woute/Documents/GitHub/deep_learning_from_scratch/logistic_regression.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m O\u001b[39m.\u001b[39;49mapply_(softmax)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call apply_() on Variable that requires grad. Use var.detach().apply_() instead."
     ]
    }
   ],
   "source": [
    "O.apply_(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X_train[:64]\n",
    "y = y_train[:64]\n",
    "\n",
    "w = torch.randn(X.shape[1], 1, requires_grad=True)\n",
    "b = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "o = X @ w + b\n",
    "\n",
    "def softmax(o):\n",
    "    return torch.exp(o) / torch.exp(o).sum()\n",
    "\n",
    "y_pred = softmax(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(o):\n",
    "    return torch.exp(o) / torch.exp(o).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.7284],\n",
       "        [  5.3626],\n",
       "        [  9.6239],\n",
       "        [ -1.8063],\n",
       "        [  0.4426],\n",
       "        [  3.1868],\n",
       "        [ -2.0599],\n",
       "        [ 16.1163],\n",
       "        [ -0.7369],\n",
       "        [  6.7454],\n",
       "        [  5.7246],\n",
       "        [  3.8791],\n",
       "        [ -2.4204],\n",
       "        [  0.6376],\n",
       "        [ -2.7021],\n",
       "        [  8.7005],\n",
       "        [ -2.1716],\n",
       "        [ -5.8198],\n",
       "        [  3.8899],\n",
       "        [ -3.4341],\n",
       "        [  9.1908],\n",
       "        [  1.2324],\n",
       "        [ -2.6439],\n",
       "        [ -4.0557],\n",
       "        [ -0.9578],\n",
       "        [  2.4787],\n",
       "        [ -0.9881],\n",
       "        [  5.9406],\n",
       "        [  4.6929],\n",
       "        [  4.0552],\n",
       "        [ -8.2735],\n",
       "        [ -0.4614],\n",
       "        [ 13.2897],\n",
       "        [  2.2910],\n",
       "        [ 10.2118],\n",
       "        [  1.0290],\n",
       "        [  2.2115],\n",
       "        [  8.1164],\n",
       "        [ -8.0071],\n",
       "        [ 10.7568],\n",
       "        [ -3.2584],\n",
       "        [  2.6659],\n",
       "        [ -8.5159],\n",
       "        [ -1.4384],\n",
       "        [ -7.9096],\n",
       "        [ -1.7909],\n",
       "        [ -4.1755],\n",
       "        [ -0.4762],\n",
       "        [  1.4304],\n",
       "        [ 11.3068],\n",
       "        [ 11.6169],\n",
       "        [ -3.3377],\n",
       "        [-11.4118],\n",
       "        [ -1.0127],\n",
       "        [ -6.9513],\n",
       "        [  2.8696],\n",
       "        [ -3.3665],\n",
       "        [ -1.5667],\n",
       "        [  4.6544],\n",
       "        [  1.4658],\n",
       "        [ -2.9701],\n",
       "        [  7.1026],\n",
       "        [  6.7471],\n",
       "        [ -3.8885]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseClassificationModel):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(in_features, 1, requires_grad=True)\n",
    "        self.b = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred = X @ self.w + self.b\n",
    "        return y_pred\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        loss = torch.mean((y_true - y_pred) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2412],\n",
       "        [-4.8160],\n",
       "        [11.7902],\n",
       "        ...,\n",
       "        [-0.7309],\n",
       "        [-4.7391],\n",
       "        [-7.6389]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ w + b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
