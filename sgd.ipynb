{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) optimizes a model by updating its parameters iteratively using a gradient computed from a randomly selected subset of data. The formula for the update step in SGD is:\n",
    "\n",
    "$$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\nabla_\\theta J(\\theta_{\\text{old}}, x^{(i)}, y^{(i)}) $$\n",
    "\n",
    "Here:\n",
    "- $ \\theta_{\\text{old}} $ represents the current parameters of the model.\n",
    "- $ \\eta $ is the learning rate, a hyperparameter that determines the size of the steps taken towards the minimum of the loss function.\n",
    "- $ \\nabla_\\theta J(\\theta_{\\text{old}}, x^{(i)}, y^{(i)}) $ is the gradient of the loss function $ J $ with respect to the parameters $ \\theta $, evaluated at the current parameter values and based on a single data point (or a small batch) $ (x^{(i)}, y^{(i)}) $.\n",
    "- $ \\theta_{\\text{new}} $ represents the updated parameters after the current iteration\n",
    "In each iteration, a data point or a small batch of data points is randomly selected, and the gradient of the loss function with respect to the model parameters is computed using only this subset. The parameters are then updated in the direction that reduces the loss, with the magnitude of the update controlled by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement SGD from scratch by implementing a function for taking a single step where we update each parameters. We also implement zero_grad, which zeroes the gradient of each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDScratch:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            # Update parameters (gradient descent)\n",
    "            for param in self.parameters:\n",
    "                param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            # Zero gradients (if they exist)\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainerScratch:\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, custom_metrics=None):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.custom_metrics = custom_metrics if custom_metrics else {}\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in self.train_dataloader:\n",
    "            inputs, targets = batch\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dataloader:\n",
    "                inputs, targets = batch\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                for name, metric in self.custom_metrics.items():\n",
    "                    metric.update(outputs, targets)\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_dataloader)\n",
    "        metrics_results = {name: metric.compute() for name, metric in self.custom_metrics.items()}\n",
    "        return avg_loss, metrics_results\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_metrics = self.validate_epoch()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            for name, value in val_metrics.items():\n",
    "                print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "            # Reset custom metrics for next epoch\n",
    "            for metric in self.custom_metrics.values():\n",
    "                metric.reset()\n",
    "        \n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
