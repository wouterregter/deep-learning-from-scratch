{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB dataset is a popular dataset used for sentiment analysis tasks. It consists of a collection of movie reviews, along with their corresponding sentiment labels (positive or negative).\n",
    "\n",
    "To work with the IMDB dataset in this Jupyter Notebook, we have already imported the necessary modules and loaded the dataset using the `torchtext.datasets.IMDB` class. The dataset has been split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "from src.data import DataLoaderScratch\n",
    "from src.trainer import TrainerScratch\n",
    "from src.optimizers import SGDScratch\n",
    "from src.losses import CrossEntropyScratch\n",
    "from src.metrics import AccuracyScratch\n",
    "from src.functions import conv2d, maxpool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels: torch.Size([8])\n",
      "Batch texts: torch.Size([8, 850])\n",
      "Batch lengths: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_iter, test_iter = list(train_iter), list(test_iter)  # Convert iterators to lists\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Build a vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Building the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"], min_freq=1)\n",
    "\n",
    "# Set the default index for unknown tokens\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Numericalize the text data\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "# Function to collate data samples into batches\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(int(_label == 'pos'))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    texts = rnn_utils.pad_sequence(text_list, batch_first=True)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    return labels, texts, lengths\n",
    "\n",
    "# Convert lists to datasets for DataLoader compatibility\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Example batch verification\n",
    "for labels, texts, lengths in train_dataloader:\n",
    "    print(f\"Batch labels: {labels.shape}\")\n",
    "    print(f\"Batch texts: {texts.shape}\")\n",
    "    print(f\"Batch lengths: {lengths.shape}\")\n",
    "    break  # To show an example batch, break here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    out = torch.maximum(x, torch.zeros(1))\n",
    "    return out\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    X_softmax = X_exp / X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_softmax\n",
    "\n",
    "def log_loss(y_pred, y):\n",
    "    y_one_hot = nn.functional.one_hot(y)\n",
    "    loss = -(y_one_hot * torch.log(y_pred)).sum(axis=1).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "vocab_size = len(vocab)\n",
    "hidden_size = 50\n",
    "sequence_length = 615\n",
    "\n",
    "E = nn.Parameter(torch.randn(vocab_size, 300) * 0.1)\n",
    "\n",
    "Wax = torch.randn(50, 300)\n",
    "Waa = torch.randn(50, 50)\n",
    "Wa = nn.Parameter(torch.hstack([Wax, Waa]) * 0.1)\n",
    "Wya = nn.Parameter(torch.randn(2, hidden_size) * 0.1)\n",
    "\n",
    "parameters = [E, Wa]\n",
    "optimizer = SGDScratch(parameters, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m softmax(torch\u001b[38;5;241m.\u001b[39mbmm(Wya_repeated, a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m log_loss(y_pred, targets)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\woute\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\woute\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        # Unpack the batch\n",
    "        targets, inputs, lengths = batch\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = inputs.shape[0]\n",
    "        # Get the maximum sequence length of the batch\n",
    "        sequence_length = max(lengths)\n",
    "\n",
    "        # One hot encode the inputs\n",
    "        Ox = nn.functional.one_hot(inputs, num_classes=vocab_size).float()\n",
    "        # Repeat the embedding matrix along the batch dimension\n",
    "        E_repeated = E.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        # Batch multuply the one hot encoded inputs with the embedding matrix\n",
    "        Ex = torch.bmm(Ox, E_repeated).squeeze(-1)\n",
    "\n",
    "        # Initialize the hidden state\n",
    "        a = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "        # Forward pass\n",
    "        for sequence_index in range(sequence_length):\n",
    "            # Get the current input\n",
    "            ex = Ex[:, sequence_index, :]\n",
    "\n",
    "            # Repeat the weigth matrix along the batch dimension\n",
    "            Wa_repeated = Wa.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "            # Calculate the new hidden state\n",
    "            a = torch.bmm(Wa_repeated, torch.hstack([a, ex]).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            # Calculate the output\n",
    "            if sequence_index == sequence_length-1:\n",
    "                Wya_repeated = Wya.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "                y_pred = softmax(torch.bmm(Wya_repeated, a.unsqueeze(-1)).squeeze(-1))\n",
    "                loss = log_loss(y_pred, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([615, 300])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x[0] @ E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.bmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 615, 100684])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[279,  13,   9,  ...,   0,   0,   0],\n",
       "        [ 51,  26, 741,  ...,   0,   0,   0],\n",
       "        [ 13,  33, 215,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 14,  10,   6,  ...,   0,   0,   0],\n",
       "        [ 14,  21,  17,  ...,   0,   0,   0],\n",
       "        [ 13,   9, 152,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.zeros(8)\n",
    "E = torch.randn() * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = texts.shape[1]\n",
    "for sequence_index in range(sequence_length):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.285222887992859\n",
      "2.1936349272727966\n",
      "2.0781480073928833\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, targets = batch\n",
    "    \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # CONV + POOL Layer 1 (sees 1x28x28 and outputs 16x14x14)\n",
    "        padding = calculate_same_padding(input_size=28, kernel_size=3, stride=1)\n",
    "        Z1 = conv2d(inputs, W1, padding=padding) + b1\n",
    "        A1 = relu(Z1)\n",
    "        P1 = maxpool2d(A1, kernel_size=2, stride=2)\n",
    "    \n",
    "        # CONV + POOL Layer 2 (sees 16x14x14 and outputs 32x7x7)\n",
    "        padding = calculate_same_padding(input_size=14, kernel_size=3, stride=1)\n",
    "        Z2 = conv2d(P1, W2, padding=padding) + b2\n",
    "        A2 = relu(Z2)\n",
    "        P2 = maxpool2d(A2, kernel_size=2, stride=2)\n",
    "    \n",
    "        # FC Layer (sees 32x7x7 and outputs 32*7*7x10)\n",
    "        P2_flat = P2.flatten(start_dim=1)\n",
    "        Z3 = P2_flat @ W3 + b3\n",
    "        y_pred = softmax(Z3)\n",
    "    \n",
    "        # Calculate Loss\n",
    "        loss = log_loss(y_pred, targets)\n",
    "    \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(total_loss / len(train_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
