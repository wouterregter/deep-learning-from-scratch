{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.data import DataLoaderScratch\n",
    "from src.trainer import TrainerScratch\n",
    "from src.optimizers import SGDScratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "mnist_testset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train = mnist_trainset.data.float() / 255.0\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "# Transform the test data\n",
    "X_val = mnist_testset.data.float() / 255.0\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "y_val = mnist_testset.targets\n",
    "\n",
    "train_dataloader = DataLoaderScratch(X_train, y_train, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoaderScratch(X_val, y_val, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a very simple simple 4x4 greyscale image with an edge down the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.]])\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(input)\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an edge detection filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "filter = torch.tensor([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(filter)\n",
    "print(filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using For Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "# Add zero padding to the input tensor\n",
    "input_padded = torch.nn.functional.pad(input, (padding, padding, padding, padding), \"constant\", 0)\n",
    "\n",
    "input_height, input_width = input_padded.shape\n",
    "filter_height, filter_width = filter.shape\n",
    "\n",
    "# Calculate the dimensions of the output tensor\n",
    "output_height = ((input_height - filter_height) // stride) + 1\n",
    "output_width = ((input_width - filter_width) // stride) + 1\n",
    "output = torch.zeros((output_height, output_width))\n",
    "\n",
    "for i in range(0, output_height):\n",
    "    for j in range(0, output_width):\n",
    "        # Apply the filter\n",
    "        output[i, j] = torch.sum(\n",
    "            input_padded[i*stride:i*stride+filter_height, j*stride:j*stride+filter_width] * filter)\n",
    "    \n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the input and filter to simulate an example with added dimensions for the images, channels and filters.\n",
    "For the inputs, we want our new dimensions to be $(batch \\_ size, channels, input \\_ height, input\\_width)$.\n",
    "For the filters, we want our new dimensions to be $(out \\_ channels, in \\_ channels, input \\_ height, input\\_width)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: torch.Size([1, 1, 6, 6])\n",
      "filters shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input = input.unsqueeze(0).unsqueeze(0)\n",
    "filter = filter.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(\"inputs shape:\", input.shape)\n",
    "print(\"filters shape:\", filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting shapes indicate that we have \n",
    "- 1 image with 1 color channel of size 6x6\n",
    "- 1 filter with 1 color channel of size 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **unfold** to take each filter position in the image, flatten it and stack it horizontally in a tensor. This way, each row contains a vector of the filter position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input: \n",
      "tensor([[[[ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.]]]])\n",
      "torch.Size([1, 1, 6, 6])\n",
      "unfolded input:\n",
      "tensor([[[ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.]]])\n",
      "torch.Size([1, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = input.shape\n",
    "out_channels, _, filter_height, filter_width = filter.shape\n",
    "\n",
    "# Padding\n",
    "input_padded = F.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "# Calculate output dimensions\n",
    "output_height = (input_height + 2*padding - filter_height) // stride + 1\n",
    "output_width = (input_width + 2*padding - filter_width) // stride + 1\n",
    "\n",
    "# Unfold input to get sliding windows\n",
    "input_unfolded = F.unfold(input_padded, (filter_height, filter_width), stride=stride).transpose(1, 2)\n",
    "\n",
    "print(\"original input: \")\n",
    "print(input)\n",
    "print(input.shape)\n",
    "print(\"unfolded input:\")\n",
    "print(input_unfolded)\n",
    "print(input_unfolded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the result, we have each 3x3 filter position stacked row-wise in the output. Now we want to multiply each row by the filter and we should have an efficient implementation of convolution. \n",
    "\n",
    "For each image we have a matrix for the input and for the filters. So, to do the matrix multiplication we will use torch.bmm which performs a batch matrix-matrix product of matrices. \n",
    "\n",
    "For each individual image, our input shape is $(filter \\_ positions, filter \\_ height * filter \\_ width)$. We want to matrix multiply each input image with the filter that is $(filter \\_ height, filter \\_ width)$. Therefore, we want to reshape the filter to make it shape $(filter \\_ height * filter \\_ width, 1)$. In this way, we get a valid matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.],\n",
      "         [ 0.],\n",
      "         [-1.],\n",
      "         [ 1.],\n",
      "         [ 0.],\n",
      "         [-1.],\n",
      "         [ 1.],\n",
      "         [ 0.],\n",
      "         [-1.]]])\n",
      "torch.Size([1, 9, 1])\n"
     ]
    }
   ],
   "source": [
    "# Reshape filter for batch matrix multiplication\n",
    "filter_reshaped = filter.view(out_channels, -1).transpose(0, 1).unsqueeze(0)\n",
    "print(filter_reshaped)\n",
    "print(filter_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.]]]])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Perform batch matrix multiplication, ensuring both tensors are 3D\n",
    "output = torch.bmm(input_unfolded, filter_reshaped.repeat(batch_size, 1, 1))\n",
    "\n",
    "# Reshape to get the final output\n",
    "output = output.transpose(1, 2).view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a correct convolution of the the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the code into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, filter, padding=0, stride=1):\n",
    "    batch_size, in_channels, input_height, input_width = input.shape\n",
    "    out_channels, _, filter_height, filter_width = filter.shape\n",
    "\n",
    "    # Padding\n",
    "    input_padded = F.pad(input, (padding, padding, padding, padding))\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    output_height = (input_height + 2*padding - filter_height) // stride + 1\n",
    "    output_width = (input_width + 2*padding - filter_width) // stride + 1\n",
    "\n",
    "    # Unfold input to get sliding windows\n",
    "    input_unfolded = F.unfold(input_padded, (filter_height, filter_width), stride=stride).transpose(1, 2)\n",
    "\n",
    "    # Reshape filter for batch matrix multiplication\n",
    "    filter_reshaped = filter.view(out_channels, -1).transpose(0, 1).unsqueeze(0)\n",
    "\n",
    "    # Perform batch matrix multiplication, ensuring both tensors are 3D\n",
    "    output = torch.bmm(input_unfolded, filter_reshaped.repeat(batch_size, 1, 1))\n",
    "\n",
    "    # Reshape to get the final output\n",
    "    output = output.transpose(1, 2).view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the function and see whether it's result is the same as PyTorch's built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the custom and built-in conv2d outputs close? True\n"
     ]
    }
   ],
   "source": [
    "# Initialize the same input and filters as for your custom conv2d function\n",
    "input = torch.randn(2, 3, 8, 8)  # batch_size=2, in_channels=3, height=8, width=8\n",
    "filter = torch.randn(4, 3, 3, 3)  # out_channels=4, in_channels=3, height=3, width=3\n",
    "\n",
    "# Custom conv2d function call\n",
    "output_custom = conv2d(input, filter, padding=0, stride=1)\n",
    "\n",
    "# Using PyTorch's built-in Conv2d\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "# Manually set the weights of the Conv2d layer to match our filters\n",
    "conv_layer.weight.data = filter\n",
    "\n",
    "# Compute the output using the built-in Conv2d\n",
    "output_builtin = conv_layer(input)\n",
    "\n",
    "# Check if the outputs are close enough\n",
    "are_close = torch.allclose(output_custom, output_builtin, atol=1e-6)\n",
    "print(f\"Are the custom and built-in conv2d outputs close? {are_close}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "mnist_testset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Transform the training data\n",
    "X_train = mnist_trainset.data.float() / 255.0\n",
    "# Add single dimension for the input channel\n",
    "X_train = X_train.unsqueeze(1)\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "# Transform the test data\n",
    "X_val = mnist_testset.data.float() / 255.0\n",
    "# Add single dimension for the input channel\n",
    "X_val = X_val.unsqueeze(1)\n",
    "y_val = mnist_testset.targets\n",
    "\n",
    "train_dataloader = DataLoaderScratch(X_train, y_train, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoaderScratch(X_val, y_val, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Batch Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch\n",
    "batch_size = 128\n",
    "perm = torch.randperm(len(X_train))\n",
    "X_batch = X_train[perm][:batch_size]\n",
    "y_batch = y_train[perm][:batch_size]\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = X_batch.shape\n",
    "filter_size = 3\n",
    "out_channels = 10 # Number of filters\n",
    "num_classes = y_train.unique().shape[0]\n",
    "\n",
    "# Initialize the conv layer weightsa and add a single channel dimension\n",
    "W1 = nn.Parameter(torch.randn(out_channels, in_channels, filter_size, filter_size) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(size=(1, out_channels, 1, 1)))\n",
    "same_padding = int((filter_size - 1)/2)\n",
    "\n",
    "# Initialize the fc layer weights\n",
    "W2 = nn.Parameter(torch.randn(size=(out_channels * input_height * input_width, num_classes)) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "parameters = [W1, b1, W2, b2]\n",
    "optimizer = SGDScratch(parameters, lr=0.1)\n",
    "\n",
    "def relu(x):\n",
    "    out = torch.maximum(x, torch.zeros(1))\n",
    "    return out\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    X_softmax = X_exp / X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_softmax\n",
    "\n",
    "# Define the log-loss\n",
    "def log_loss(y_pred, y):\n",
    "    y_one_hot = nn.functional.one_hot(y)\n",
    "    loss = -(y_one_hot * torch.log(y_pred)).sum(axis=1).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "Z1 = conv2d(X_batch, W1, padding=padding) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = A1.flatten(1) @ W2 + b2\n",
    "y_pred = softmax(Z2)\n",
    "\n",
    "# Calculate Loss\n",
    "loss = log_loss(y_pred, y_batch)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3040, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
