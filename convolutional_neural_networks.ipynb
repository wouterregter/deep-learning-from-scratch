{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.data import DataLoaderScratch\n",
    "from src.trainer import TrainerScratch\n",
    "from src.optimizers import SGDScratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "mnist_testset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train = mnist_trainset.data.float() / 255.0\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "# Transform the test data\n",
    "X_val = mnist_testset.data.float() / 255.0\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "y_val = mnist_testset.targets\n",
    "\n",
    "train_dataloader = DataLoaderScratch(X_train, y_train, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoaderScratch(X_val, y_val, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a very simple simple 4x4 greyscale image with an edge down the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.],\n",
      "        [ 10.,  10.,  10., -10., -10., -10.]])\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "    [10, 10, 10, -10, -10, -10],\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(input)\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an edge detection filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "filter = torch.tensor([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(filter)\n",
    "print(filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using For Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.],\n",
      "        [ 0., 60., 60.,  0.]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "# Add zero padding to the input tensor\n",
    "input_padded = torch.nn.functional.pad(input, (padding, padding, padding, padding), \"constant\", 0)\n",
    "\n",
    "input_height, input_width = input_padded.shape\n",
    "filter_height, filter_width = filter.shape\n",
    "\n",
    "# Calculate the dimensions of the output tensor\n",
    "output_height = ((input_height - filter_height) // stride) + 1\n",
    "output_width = ((input_width - filter_width) // stride) + 1\n",
    "output = torch.zeros((output_height, output_width))\n",
    "\n",
    "for i in range(0, output_height):\n",
    "    for j in range(0, output_width):\n",
    "        # Apply the filter\n",
    "        output[i, j] = torch.sum(\n",
    "            input_padded[i*stride:i*stride+filter_height, j*stride:j*stride+filter_width] * filter)\n",
    "    \n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the input and filter to simulate an example with added dimensions for the images, channels and filters.\n",
    "For the inputs, we want our new dimensions to be $(batch \\_ size, channels, input \\_ height, input\\_width)$.\n",
    "For the filters, we want our new dimensions to be $(out \\_ channels, in \\_ channels, input \\_ height, input\\_width)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: torch.Size([1, 1, 6, 6])\n",
      "filters shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input = input.unsqueeze(0).unsqueeze(0)\n",
    "filter = filter.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(\"inputs shape:\", input.shape)\n",
    "print(\"filters shape:\", filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting shapes indicate that we have \n",
    "- 1 image with 1 color channel of size 6x6\n",
    "- 1 filter with 1 color channel of size 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **unfold** to take each filter position in the image, flatten it and stack it horizontally in a tensor. This way, each row contains a vector of the filter position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input: \n",
      "tensor([[[[ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.],\n",
      "          [ 10.,  10.,  10., -10., -10., -10.]]]])\n",
      "torch.Size([1, 1, 6, 6])\n",
      "unfolded input:\n",
      "tensor([[[ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "         [ 10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.],\n",
      "         [ 10.,  10., -10.,  10.,  10., -10.,  10.,  10., -10.],\n",
      "         [ 10., -10., -10.,  10., -10., -10.,  10., -10., -10.],\n",
      "         [-10., -10., -10., -10., -10., -10., -10., -10., -10.]]])\n",
      "torch.Size([1, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = input.shape\n",
    "out_channels, _, filter_height, filter_width = filter.shape\n",
    "\n",
    "# Add padding to the input\n",
    "input_padded = torch.nn.functional.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "# Calculate the output dimensions\n",
    "output_height = (input_height + 2 * padding - filter_height) // stride + 1\n",
    "output_width = (input_width + 2 * padding - filter_width) // stride + 1\n",
    "\n",
    "# Unfold the input tensor to get all the sliding windows\n",
    "input_unfolded = input_padded.unfold(2, filter_height, stride).unfold(3, filter_width, stride)\n",
    "input_unfolded = input_unfolded.contiguous().view(batch_size, output_height * output_width, in_channels * filter_height * filter_width)\n",
    "\n",
    "print(\"original input: \")\n",
    "print(input)\n",
    "print(input.shape)\n",
    "print(\"unfolded input:\")\n",
    "print(input_unfolded)\n",
    "print(input_unfolded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to multiply each row by the filter and we should have an efficient implementation of convolution. \n",
    "\n",
    "For each image we have a matrix for the input and for the filters. So, to do the matrix multiplication we will use torch.bmm which performs a batch matrix-matrix product of matrices. \n",
    "\n",
    "For each individual image, our input shape is $(filter \\_ positions, filter \\_ height * filter \\_ width)$. We want to matrix multiply each input image with the filter that is $(filter \\_ height, filter \\_ width)$. Therefore, we want to reshape the filter to make it shape $(filter \\_ height * filter \\_ width, 1)$. In this way, we get a valid matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 9])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the kernel for matrix multiplication\n",
    "filter_reshaped = filter.view(out_channels, -1).unsqueeze(1)\n",
    "filter_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.],\n",
      "          [ 0., 60., 60.,  0.]]]])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the kernel for matrix multiplication\n",
    "filter_reshaped = filter.view(out_channels, -1, 1)\n",
    "\n",
    "# Perform batch matrix multiplication\n",
    "# Result shape: (batch_size, out_channels, output_height * output_width)\n",
    "output = torch.bmm(input_unfolded, filter_reshaped)\n",
    "\n",
    "# Reshape to (batch_size, out_channels, output_height, output_width)\n",
    "output = output.view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a correct convolution of the the image. Let's now put all of this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, filter, padding=0, stride=1):\n",
    "    # Assuming input shape is (batch_size, in_channels, height, width)\n",
    "    # and filter shape is (out_channels, in_channels, kernel_height, kernel_width)\n",
    "    \n",
    "    batch_size, in_channels, input_height, input_width = input.shape\n",
    "    out_channels, _, filter_height, filter_width = filter.shape\n",
    "    \n",
    "    # Add padding to the input\n",
    "    input_padded = torch.nn.functional.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = (input_height + 2 * padding - filter_height) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - filter_width) // stride + 1\n",
    "    \n",
    "    # Unfold the input tensor to get all the sliding windows\n",
    "    input_unfolded = input_padded.unfold(2, filter_height, stride).unfold(3, filter_width, stride)\n",
    "    input_unfolded = input_unfolded.contiguous().view(batch_size, output_height * output_width, in_channels * filter_height * filter_width)\n",
    "    \n",
    "    # Reshape the kernel for matrix multiplication\n",
    "    filter_reshaped = filter.view(out_channels, -1, 1)\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    # Result shape: (batch_size, out_channels, output_height * output_width)\n",
    "    output = torch.bmm(input_unfolded, filter_reshaped)\n",
    "\n",
    "    # Reshape to (batch_size, out_channels, output_height, output_width)\n",
    "    output = output.view(batch_size, out_channels, output_height, output_width)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0., 60., 60.,  0.],\n",
       "          [ 0., 60., 60.,  0.],\n",
       "          [ 0., 60., 60.,  0.],\n",
       "          [ 0., 60., 60.,  0.]]]])"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(input, filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
