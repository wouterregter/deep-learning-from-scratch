{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Theory\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used in machine learning to train models. It updates the model's parameters iteratively using a gradient computed from a randomly selected subset of data.\n",
    "\n",
    "The formula for the update step in SGD is:\n",
    "\n",
    "$$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\nabla_\\theta J(\\theta_{\\text{old}}, x^{(i)}, y^{(i)}) $$\n",
    "\n",
    "Here:\n",
    "- $ \\theta_{\\text{old}} $ represents the current parameters of the model.\n",
    "- $ \\eta $ is the learning rate, a hyperparameter that determines the size of the steps taken towards the minimum of the loss function.\n",
    "- $ \\nabla_\\theta J(\\theta_{\\text{old}}, x^{(i)}, y^{(i)}) $ is the gradient of the loss function $ J $ with respect to the parameters $ \\theta $, evaluated at the current parameter values and based on a single data point (or a small batch) $ (x^{(i)}, y^{(i)}) $.\n",
    "- $ \\theta_{\\text{new}} $ represents the updated parameters after the current iteration.\n",
    "\n",
    "In each iteration, a data point or a small batch of data points is randomly selected, and the gradient of the loss function with respect to the model parameters is computed using only this subset. The parameters are then updated in the direction that reduces the loss, with the magnitude of the update controlled by the learning rate.\n",
    "\n",
    "SGD is particularly useful when dealing with large datasets, as it allows for faster convergence compared to batch gradient descent. However, it introduces some level of randomness due to the random selection of data points, which can lead to noisy updates.\n",
    "\n",
    "It is important to tune the learning rate carefully, as a high learning rate can cause the algorithm to overshoot the minimum of the loss function, while a low learning rate can result in slow convergence.\n",
    "\n",
    "SGD is a widely used optimization algorithm in various machine learning models, including neural networks, linear regression, and logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows the implementation of the `SGDScratch` class, which is a custom implementation of the Stochastic Gradient Descent (SGD) optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDScratch:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            # Update parameters (gradient descent)\n",
    "            for param in self.parameters:\n",
    "                param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            # Zero gradients (if they exist)\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block defines a class called `TrainerScratch` that is used for training and evaluating a model using the Stochastic Gradient Descent (SGD) optimization algorithm. Here's a breakdown of the class and its methods:\n",
    "\n",
    "- `__init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, custom_metrics=None)`: This is the constructor method that initializes the `TrainerScratch` object. It takes the following parameters:\n",
    "  - `model`: The model to be trained and evaluated.\n",
    "  - `train_dataloader`: The dataloader for the training data.\n",
    "  - `val_dataloader`: The dataloader for the validation data.\n",
    "  - `optimizer`: The optimizer used for updating the model's parameters.\n",
    "  - `criterion`: The loss function used for computing the loss.\n",
    "  - `custom_metrics`: Optional custom metrics to be computed during validation.\n",
    "\n",
    "- `train_epoch(self)`: This method performs one epoch of training. It iterates over the training data, computes the loss, performs backpropagation, and updates the model's parameters. It returns the average loss for the epoch.\n",
    "\n",
    "- `validate_epoch(self)`: This method performs one epoch of validation. It iterates over the validation data, computes the loss, and computes any custom metrics specified. It returns the average loss and the computed metrics for the epoch.\n",
    "\n",
    "- `fit(self, num_epochs)`: This method trains the model for the specified number of epochs. It iterates over the epochs, performs training and validation, prints the training and validation losses, and plots the training and validation losses using matplotlib.\n",
    "\n",
    "The `TrainerScratch` class provides a convenient way to train and evaluate models using SGD in a Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TrainerScratch:\n",
    "    \"\"\"\n",
    "    A class that performs training and validation using a custom model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        train_dataloader (torch.utils.data.DataLoader): The dataloader for training data.\n",
    "        val_dataloader (torch.utils.data.DataLoader): The dataloader for validation data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        criterion (torch.nn.Module): The loss function used for training.\n",
    "        custom_metrics (dict, optional): A dictionary of custom metrics to evaluate during validation. \n",
    "            The keys are the names of the metrics and the values are instances of custom metric classes.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        train_dataloader (torch.utils.data.DataLoader): The dataloader for training data.\n",
    "        val_dataloader (torch.utils.data.DataLoader): The dataloader for validation data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        criterion (torch.nn.Module): The loss function used for training.\n",
    "        custom_metrics (dict): A dictionary of custom metrics to evaluate during validation. \n",
    "            The keys are the names of the metrics and the values are instances of custom metric classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, custom_metrics=None):\n",
    "        # Initialize the TrainerScratch object with the provided arguments\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.custom_metrics = custom_metrics if custom_metrics else {}\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch using the training data.\n",
    "\n",
    "        Returns:\n",
    "            float: The average loss for the epoch.\n",
    "        \"\"\"\n",
    "        # Set the model to training mode\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in self.train_dataloader:\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        \"\"\"\n",
    "        Validates the model for one epoch using the validation data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the average loss for the epoch and a dictionary of metric results.\n",
    "        \"\"\"\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dataloader:\n",
    "                inputs, targets = batch\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Update the custom metrics\n",
    "                for name, metric in self.custom_metrics.items():\n",
    "                    metric.update(outputs, targets)\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_dataloader)\n",
    "\n",
    "        # Compute the metric results\n",
    "        metrics_results = {name: metric.compute() for name, metric in self.custom_metrics.items()}\n",
    "        return avg_loss, metrics_results\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Trains the model for the specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            num_epochs (int): The number of epochs to train the model for.\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train the model for one epoch\n",
    "            train_loss = self.train_epoch()\n",
    "\n",
    "            # Validate the model for one epoch\n",
    "            val_loss, val_metrics = self.validate_epoch()\n",
    "            \n",
    "            # Append the losses to the lists\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Print the epoch information\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            for name, value in val_metrics.items():\n",
    "                print(f\"Validation {name}: {value:.4f}\")\n",
    "\n",
    "            # Reset custom metrics for next epoch\n",
    "            for metric in self.custom_metrics.values():\n",
    "                metric.reset()\n",
    "        \n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
